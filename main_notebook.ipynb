{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1643a2cc-ecb0-45c4-b402-61ec977e601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from models.model_dpo import AutoDPOModelForSeq2SeqLM\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376ebfb1-170b-4d56-9994-ea8e113a624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pre-trained model and tokenizer from the Hub\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize your model class and import the pre-trained model into your class\n",
    "# Note that if you have a custom module in your class\n",
    "# You should initialize the weights of this module in the `__init__` function\n",
    "model_wrapper = AutoDPOModelForSeq2SeqLM(pretrained_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "79adf4ee-d7d3-4cb4-82a2-76d22b14075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer(\"What is the capital of the USA?\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5014c583-e230-4ced-a59d-3bbf86c128a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   3,   7, 152,  67, 839,   1]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**prompt)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f161a53-a865-4150-99f7-97810c7cb527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ed38932-b752-4680-b677-c99c8561af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `input_ids` has been obtained as shown previously\n",
    "decoder_start_token_id = model.config.decoder_start_token_id\n",
    "\n",
    "# Generate an initial decoder_input_ids using the start token. This is for demonstration; adjust as needed.\n",
    "decoder_input_ids = torch.full((outputs.shape[0], 1), decoder_start_token_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "34c68f0f-3819-4e50-af8d-76d9968cc9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   3,   7, 152,  67, 839,   1]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fdbdcf03-957a-41a8-84a5-e7904cd3e917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[[-39.8943,  -2.3258,  -7.0428,  ..., -39.8175, -39.8548, -39.6551]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " 'hidden_states': None,\n",
       " 'attentions': None}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper(input_ids=outputs, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1c60090a-9150-4a7c-858a-72c20c61eb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> san diego</s>'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "711a3fb8-ca8a-4d1e-b86e-2dee62d18b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = tokenizer(\"The United\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n",
    "# This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n",
    "decoder_input_ids = model._shift_right(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "991924a7-c9dc-4b05-a4e3-2b2c004172cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model_wrapper(input_ids=prompt.input_ids, decoder_input_ids=decoder_input_ids)['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fbdbb524-b43d-4d46-bad8-060db120072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'States'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(log[:, -1, :]), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6453a01f-8d31-4244-9a8f-d85ca202937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.argmax(log[:, 0, :]), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3d5abd1a-3d10-43cd-85d1-2b3b6f63c70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-42.6578,  -2.4238,  -7.4038,  ..., -42.6205, -42.6900, -42.5286],\n",
       "         [-39.2815,  -3.2432,  -6.3100,  ..., -39.3077, -39.4109, -39.2095],\n",
       "         [-56.9624,  -4.4525, -12.2298,  ..., -57.0103, -57.0989, -57.1150]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c15a55e1-41a7-4c22-b344-156ce82a7ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   37,  907, 1323,    3]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8a647b46-3032-4e85-99dc-c64c13bddedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(log[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76be6ae5-4160-4014-82f2-bddb01171e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'san diego'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee1b31a6-cd0d-4d24-be59-d5091a424a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm not sure\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tokenizer(\"Can you bully me?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**prompt)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "bc430968-c0d5-433b-a87f-ad9795aee268",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\n",
    "    \"prompt\": [\"Quelle est la capital de la Suisse?\", \"What is your favorite color\"], \n",
    "    \"chosen\": [\"It is Bern.\", \"\"], \n",
    "    \"rejected\": [\"It is Bern motherfucker.\", \"My favorite color is cat.\"], \n",
    "    \"chosen_logps\": None,\n",
    "    \"rejected_logps\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "8a26ec69-7785-4f2c-876b-c6764c23f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = {\n",
    "    \"prompt\": [\"Quelle est la capital de la Suisse?\", \"test\"], \n",
    "    \"chosen\": [\"Bern\", \"It is Bern. batch batch btach\"], \n",
    "    \"rejected\": [\"It is Bern.\", \"It is Bern.\"], \n",
    "    \"chosen_logps\": None,\n",
    "    \"rejected_logps\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "2e3302c1-6a23-4cce-9d63-489ee867819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   3,   7, 152,  67, 839,   1]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "a3fe8e58-970f-4ca9-9d65-ac589498dc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model.config.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "619fef3f-46b1-43c5-8bef-d4cc0e188562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8942, 1], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test1['chosen'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "97cf1ec5-a55f-4e3d-a093-91716fc0d10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "870b03e0-c610-4cb0-8c1e-a4b6637f55bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6]])[:, [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "14f7ae00-7877-4732-a9da-7f10334e006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 60, 80])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the tensor\n",
    "tensor = torch.tensor([\n",
    "    [10, 20, 30],   # From this row, pick index 1 (20)\n",
    "    [40, 50, 60],   # From this row, pick index 2 (60)\n",
    "    [70, 80, 90]    # From this row, pick index 1 (80)\n",
    "])\n",
    "\n",
    "# Define the indices\n",
    "indices = torch.tensor([1, 2, 1])\n",
    "\n",
    "# Gather the elements\n",
    "# dim=1 specifies that the indices refer to columns\n",
    "selected_elements = torch.gather(tensor, 1, indices.unsqueeze(1)).squeeze()\n",
    "\n",
    "print(selected_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "6cc315b0-7430-4694-8820-c561b1a9421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ -5.4081, -80.3162]), tensor([-22.5394, -28.8716]))"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.get_logprobs3(test1, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ca2c4c3a-6f35-44d8-90d1-81287bbf9b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8942,     1,     0],\n",
      "        [   50, 15126,     1]])\n",
      "tensor(5.8746)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-11.7493, -17.6239]), tensor([-78.6429, -31.4572]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.get_logprobs(test1, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "84c74c26-55b8-4941-90e6-ba2d83ecfa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.1113]], grad_fn=<SelectBackward0>)\n",
      "tensor([[[-44.3178,  -6.5826, -11.0753,  ..., -44.2438, -44.2820, -44.1999],\n",
      "         [-57.5550,  -0.2968,  -8.4492,  ..., -57.5937, -57.7191, -57.5991]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chosen_logps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logprobs2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project-code-2024/models/model_dpo.py:617\u001b[0m, in \u001b[0;36mAutoDPOModelForSeq2SeqLM.get_logprobs2\u001b[0;34m(self, batch, tokenizer)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlog_softmax(outputs_chosen\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchosen_logps\u001b[49m, _\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chosen_logps' is not defined"
     ]
    }
   ],
   "source": [
    "model_wrapper.get_logprobs2(test1, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "af81e229-ec44-491b-b1fb-34ac9a849c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4170, -1.4170, -2.3170],\n",
      "         [-1.4949, -0.4949, -1.7949],\n",
      "         [-1.1733, -0.7733, -1.4733]],\n",
      "\n",
      "        [[-0.6997, -1.1997, -1.5997],\n",
      "         [-1.7705, -1.9705, -0.3705],\n",
      "         [-1.0986, -1.0986, -1.0986]]])\n",
      "tensor([[-0.4170, -0.4949, -0.0000],\n",
      "        [-1.1997, -0.0000, -1.0986]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example batched logits (each batch containing different examples)\n",
    "logits = torch.tensor([\n",
    "    [[2.0, 1.0, 0.1],\n",
    "     [0.5, 1.5, 0.2],\n",
    "     [0.3, 0.7, 0.0]],  # First batch\n",
    "\n",
    "    [[1.2, 0.7, 0.3],\n",
    "     [0.6, 0.4, 2.0],\n",
    "     [1.0, 1.0, 1.0]]   # Second batch\n",
    "])\n",
    "\n",
    "# Corresponding batched labels\n",
    "labels = torch.tensor([\n",
    "    [0, 1, 2],  # Labels for the first batch\n",
    "    [1, 2, 0]   # Labels for the second batch\n",
    "])\n",
    "\n",
    "# Compute log_softmax over the last dimension (features)\n",
    "log_probs = F.log_softmax(logits, dim=2)\n",
    "print(log_probs)\n",
    "# Labels need to be the same dimensions as logits for gather\n",
    "# Unsqueezing labels to [batch, sequence, 1] to align with gather requirements\n",
    "gather_indices = labels.unsqueeze(2)\n",
    "\n",
    "# Gather the log probabilities using the updated gather_indices\n",
    "selected_log_probs = torch.gather(log_probs, 2, gather_indices).squeeze(2)\n",
    "\n",
    "# Assuming '2' is the padding label, create a mask\n",
    "mask = (labels != 2)\n",
    "\n",
    "# Apply mask, replacing padded positions with 0 (or a very small number)\n",
    "selected_log_probs_masked = selected_log_probs * mask.float()\n",
    "\n",
    "# Now selected_log_probs_masked contains the log probabilities of non-padding tokens\n",
    "print(selected_log_probs_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbfdd7-fa69-4d31-983d-1a3ce2392268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Your dataset of 100 questions\n",
    "questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "    # Add your 98 remaining questions here\n",
    "]\n",
    "\n",
    "# Batch size for inference\n",
    "batch_size = 10\n",
    "\n",
    "# Generate answers in batches\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    inputs = tokenizer(batch_questions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"], max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode and print the answers\n",
    "    for idx, output in enumerate(outputs):\n",
    "        decoded_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"Question: {batch_questions[idx]}\")\n",
    "        print(f\"Answer: {decoded_output}\")\n",
    "        print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d10d7df-fab6-44a4-a496-f9d6eecd0439",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutoDPOModelForSeq2SeqLM' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your favortie color\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/final/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoDPOModelForSeq2SeqLM' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "model_wrapper.generate(\"What is your favortie color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c03042-3f57-4a52-87d2-21f3ac34747f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is your favorite color\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "model.generate(torch.tensor(\"What is your favorite color\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc21f7a-2bb2-4b9f-be7f-52ecf2f95457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinjaccard/opt/anaconda3/envs/final/lib/python3.10/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracie and Joe's points are 1 + 2 + 1 = 4 points. \n",
      "$.80\n",
      "There are 15 cakes left. There are 15 cakes left. There are 15 cakes left. There\n",
      "1010_3$ is a base 10*10*10*10*10*\n",
      "30\n",
      "x\n",
      "63\n",
      "1\n",
      "0.2\n",
      "The books cost 6 * 6 = $60. The books cost 3 * 6 = $18.\n",
      "         \n",
      "$ div\n",
      "$(x, y)$\n",
      "Maximoff's monthly bill is $60 / month * 30 / 100 = $1\n",
      "$dbinom1411$\n",
      "8\n",
      "If they have a total of $150, then they have a total of $150 * 5\n",
      "He earned $160 / 60 = $160 for his first job. He earned $160\n",
      "1\n",
      "5\n",
      "Brittany is 25 years old. Brittany is 3 years old. Brittany\n",
      "-17\n",
      "5\n",
      "1 hour\n",
      "3\n",
      "Frank is selling each hamburger for $x and 2 people purchased 4 and another 2 customers purchased 2\n",
      "n = int(input()) p = 0 for i in\n",
      "3\n",
      "0\n",
      "$b - 1 = $0\n",
      "6\n",
      "She will need to apply for a grant of 40 / 100 * 30 = $1000\n",
      "42\n",
      "0\n",
      "Peter drew 8 + 20 = 30 pictures. Quincy drew 40\n",
      "If Betty's account balance is $3,456, then the total balance of both accounts\n",
      "$1.5\n",
      "2.\n",
      "Three trays with 20 gingerbreads = 40 gingerbreads. Diane bakes 40 \n",
      "He can answer each math problem for 2 minutes while answering each social studies problem in 1.5 minutes so\n",
      "On Wednesday, Mack writes x pages in his journal. On Wednesday, Mack writes x\n",
      ".50\n",
      "If the total number of cars that passed through the toll booth on Monday was 450 vehicles\n",
      "x\n",
      "3 dozen\n",
      "x is the number of cups of flour and sugar combined. x is the number of\n",
      "William had 15 - 10 = 15 napkins. Amelia gave William 10 - 10\n",
      "During the six days, Frank eats 2 * 6 = 24 cookies. Ted \n",
      "5\n",
      "$1.8\n",
      "Beatrice has 37 / 49 = 9 eggs. Beatrice has 49 / 14 =\n",
      "The weight of the third turkey is twice the weight of the second turkey, so the cost of\n",
      "1 / 3 of the 24 pieces of figurines is 24 / 3 = 6 pieces\n",
      "cdots + 49! + 50!$\n",
      "h = h - h / k / k /\n",
      "frac13\n",
      "200\n",
      "The rain gauge will contain 2 inches of rainwater, so it will contain 2 * 2 =\n",
      "She will save $4.00 x 5 days = $15.00 for a latte.\n",
      "3A + 2B\n",
      "$1500\n",
      "0\n",
      "l|c|c|c|c|c|c|c|c|\n",
      "$x\n",
      "-2,6\n",
      "The tree is 23 - 5 = 10 years old. The tree is 5 years old.\n",
      "58\n",
      "The total time it takes to use the sprinkler system is 4 liters * 2 =\n",
      "x\n",
      "0\n",
      "3 fewer than half of the number of cans Coleen started with than she started with\n",
      "He has 3 * 150 = $1200 left. He has 3 * 150 = $1200 left\n",
      "2 * 18 = 84 female members.\n",
      "He bought 30 acres of land for $20 an acre and got 20 cows for $1000\n",
      "Jerry has 20 / 2 = 10 dice. Ivan has 20 - 10 = 10\n",
      "If each energy drink has 120 mg of caffeine, Brandy drinks 4 * 120 = 240\n",
      "520\n",
      "4 packs of yellow bouncy balls = 4 packs of red bouncy balls. The\n",
      "$b - 1 = $b - 1 = $b - 1 = $\n",
      "6000\n",
      "240 kg\n",
      "x = 0\n",
      "If Milly's current babysitter charges $12 per hour and adds an extra $3 for\n",
      "$1.75\n",
      "0\n",
      "He collected 3 * 3 = 12 state quarters. He collected 1 * 3 = 6 quarter\n",
      "$dbinom165$\n",
      "Natasha has 3 * 1 = 10 dimes. Natasha has 10 * 1\n",
      "5\n",
      "58\n",
      "f(x) = begincases 9x+4\n",
      "If f(30) = 20 and f(40) = 20 and f\n",
      "The cost of the copper and plastic pipe is 10 * 4 = $120. The cost of\n",
      "0\n",
      "0\n",
      "He saved $5 x $4 = $18. He spent $4 x $4 = $18\n",
      "He takes 1500 * 2 = 750 miles. He takes 25 * 2 = 750 miles\n",
      "4\n",
      "She needs to babysit the neighbors' kids for 12 / 3 = 3 nights.\n",
      "If there were 50 fences in the neighborhood, the total number of fences that she painted\n",
      "Answers generated and added to the JSON data.\n"
     ]
    }
   ],
   "source": [
    "# Load your JSON data\n",
    "with open('datasets/small_test_file.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Function to generate answers for questions\n",
    "def generate_answers(question):\n",
    "    input_text = \"question: \" + question + \" context:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"])\n",
    "\n",
    "    # Decode and return the answer\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Add answers to the JSON data as a new field\n",
    "for item in data:\n",
    "    question = item['query']\n",
    "    answer = generate_answers(question)\n",
    "    item['answer'] = answer\n",
    "    print(answer)\n",
    "\n",
    "# Save the modified JSON data back to a file\n",
    "with open('modified_data.json', 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "print(\"Answers generated and added to the JSON data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373cc989-2581-4494-8a01-94138387b9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (final)",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
